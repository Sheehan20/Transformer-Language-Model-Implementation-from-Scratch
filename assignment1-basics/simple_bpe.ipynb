{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e4aa255",
   "metadata": {},
   "source": [
    "# Train BPE tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb2536cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "from typing import Dict, Tuple, List\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17c79c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"\"\"\n",
    "low low low low low <|endoftext|>\n",
    "lower lower widest widest widest <|endoftext|>\n",
    "newest newest newest newest newest newest \n",
    "\"\"\"\n",
    "special_tokens = [\"<|endoftext|>\"]\n",
    "PAT = r\"\\S+\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62337013",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_vocab(special_tokens: List[bytes]) -> Dict[int, bytes]:\n",
    "    vocab = {i: bytes([i]) for i in range(256)}  # ASCII characters\n",
    "    for i, token in enumerate(special_tokens, start=256):\n",
    "        vocab[i] = token\n",
    "\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a6c3676",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_bytes(word: str) -> List[bytes]:\n",
    "    \"\"\"\n",
    "    Convert a word to bytes.\n",
    "    \"\"\"\n",
    "    byte_ids = [bytes([b]) for b in word.encode(\"utf-8\")]\n",
    "\n",
    "    return byte_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdd07cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_special_tokens(\n",
    "    text: str, special_tokens: list[str], include_special: bool = False\n",
    ") -> List[str]:\n",
    "    special_tokens_sorted = sorted(special_tokens, key=len, reverse=True)\n",
    "    pattern = \"|\".join(re.escape(t) for t in special_tokens_sorted)\n",
    "\n",
    "    if include_special:\n",
    "        special_chunks = re.split(f\"({pattern})\", text)\n",
    "    else:\n",
    "        # Split without capturing the special tokens\n",
    "        special_chunks = re.split(pattern, text)\n",
    "\n",
    "    return special_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29a1e9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_tokenize_string(\n",
    "    s: str, special_tokens: list[str], include_special: bool = False\n",
    ") -> Dict[Tuple[bytes], int]:\n",
    "    \"\"\"\n",
    "    Pre-tokenize a string into bytes.\n",
    "    \"\"\"\n",
    "\n",
    "    word_counter = Counter()\n",
    "    special_chunks = split_by_special_tokens(s, special_tokens, include_special)\n",
    "\n",
    "    for chunk in special_chunks:\n",
    "        if chunk in special_tokens:\n",
    "            if include_special:\n",
    "                token = tuple(word_to_bytes(chunk))\n",
    "                word_counter[token] += 1\n",
    "        else:\n",
    "            for match in re.finditer(PAT, chunk):\n",
    "                word = match.group(0)\n",
    "                token = tuple(word_to_bytes(word))\n",
    "                word_counter[token] += 1\n",
    "\n",
    "    return word_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2007122b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_counts(\n",
    "    word_counter: Dict[Tuple[bytes], int],\n",
    ") -> Dict[Tuple[bytes, bytes], int]:\n",
    "    \"\"\"\n",
    "    Count pairs of bytes in the word counter.\n",
    "    \"\"\"\n",
    "    pairs: Dict[Tuple[bytes, bytes], int] = {}\n",
    "    for token, freq in word_counter.items():\n",
    "        for i in range(len(token) - 1):\n",
    "            pair = (token[i], token[i + 1])\n",
    "            pairs[pair] = pairs.get(pair, 0) + freq\n",
    "\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def get_most_frequent_pair(\n",
    "    pairs: Dict[Tuple[bytes, bytes], int],\n",
    ") -> Tuple[bytes, bytes]:\n",
    "    max_freq = max(pairs.values())\n",
    "    candidates = [pair for pair, freq in pairs.items() if freq == max_freq]\n",
    "    res = max(candidates)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "130b228e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_pair_to_vocab(\n",
    "    vocab: Dict[int, bytes], pair: Tuple[bytes, bytes], vocab_inv: Dict[bytes, int]\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Add a new pair to the vocabulary.\n",
    "    \"\"\"\n",
    "    index = len(vocab)\n",
    "    s = vocab[vocab_inv[pair[0]]] + vocab[vocab_inv[pair[1]]]\n",
    "    vocab[index] = s\n",
    "    vocab_inv[vocab[index]] = index\n",
    "\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9d31bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "\n",
    "\n",
    "def merge_pair(\n",
    "    word_counter: Dict[Tuple[bytes], int], pair: Tuple[bytes, bytes]\n",
    ") -> Tuple[Dict[Tuple[bytes], int], Dict]:\n",
    "    \"\"\"\n",
    "    Merge a pair of bytes in the word counter.\n",
    "    \"\"\"\n",
    "    new_word_counter = Counter()\n",
    "    updated_pair_counts = defaultdict(int)\n",
    "\n",
    "    for token, freq in word_counter.items():\n",
    "        new_token = []\n",
    "        i = 0\n",
    "        while i < len(token):\n",
    "            if i < len(token) - 1 and (token[i], token[i + 1]) == pair:\n",
    "                new_token.append(token[i] + token[i + 1])\n",
    "                i += 2\n",
    "            else:\n",
    "                new_token.append(token[i])\n",
    "                i += 1\n",
    "\n",
    "        new_word_counter[tuple(new_token)] += freq\n",
    "\n",
    "        for j in range(len(new_token) - 1):\n",
    "            new_pair = (new_token[j], new_token[j + 1])\n",
    "            updated_pair_counts[new_pair] += freq\n",
    "\n",
    "    return new_word_counter, updated_pair_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7660d44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_and_convert_special_tokens(\n",
    "    special_tokens: List[str] | List[bytes],\n",
    ") -> List[bytes]:\n",
    "    \"\"\"\n",
    "    Check if special tokens are in the vocabulary and convert them to bytes.\n",
    "    \"\"\"\n",
    "    if not all(isinstance(token, bytes) for token in special_tokens):\n",
    "        special_tokens_bytes = [\n",
    "            token.encode(\"utf-8\") for token in special_tokens if isinstance(token, str)\n",
    "        ]\n",
    "\n",
    "    return special_tokens_bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4287bb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bpe(\n",
    "    string: str = string,\n",
    "    vocab_size=263,\n",
    "    special_tokens: List[str] = special_tokens,\n",
    "):\n",
    "    special_tokens_bytes = check_and_convert_special_tokens(special_tokens)\n",
    "\n",
    "    vocab = initialize_vocab(special_tokens_bytes)\n",
    "    vocab_inv = {v: k for k, v in vocab.items()}\n",
    "\n",
    "    merges: List[Tuple[bytes, bytes]] = []\n",
    "\n",
    "    word_counter = pre_tokenize_string(string, special_tokens, include_special=False)\n",
    "    pairs_freqs = pair_counts(word_counter)\n",
    "\n",
    "    num_merges = vocab_size - len(vocab)\n",
    "    for _ in range(num_merges):\n",
    "\n",
    "        most_common_pair = get_most_frequent_pair(pairs_freqs)\n",
    "\n",
    "        new_index = add_pair_to_vocab(vocab, most_common_pair, vocab_inv)\n",
    "        merges.append(most_common_pair)\n",
    "\n",
    "        word_counter, pairs_freqs = merge_pair(word_counter, most_common_pair)\n",
    "\n",
    "    return vocab, merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5bbb7a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, merge = train_bpe(vocab_size=269)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "535ef56b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(b's', b't'),\n",
       " (b'e', b'st'),\n",
       " (b'o', b'w'),\n",
       " (b'l', b'ow'),\n",
       " (b'w', b'est'),\n",
       " (b'n', b'e'),\n",
       " (b'ne', b'west'),\n",
       " (b'w', b'i'),\n",
       " (b'wi', b'd'),\n",
       " (b'wid', b'est'),\n",
       " (b'low', b'e'),\n",
       " (b'lowe', b'r')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3db717",
   "metadata": {},
   "source": [
    "# BPE Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3aef1031",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {\n",
    "    0: b\" \",\n",
    "    1: b\"a\",\n",
    "    2: b\"c\",\n",
    "    3: b\"e\",\n",
    "    4: b\"h\",\n",
    "    5: b\"t\",\n",
    "    6: b\"th\",\n",
    "    7: b\" c\",\n",
    "    8: b\" a\",\n",
    "    9: b\"the\",\n",
    "    10: b\"at\",\n",
    "}\n",
    "string = \"the cat ate\"\n",
    "merges = [(b\"t\", b\"h\"), (b\" \", b\"c\"), (b\" \", \"a\"), (b\"th\", b\"e\"), (b\" a\", b\"t\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c5e4dc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'List' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msplit_by_special_tokens\u001b[39m(\n\u001b[32m      2\u001b[39m     text: \u001b[38;5;28mstr\u001b[39m, special_tokens: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m ) -> \u001b[43mList\u001b[49m[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m      4\u001b[39m     special_tokens_sorted = \u001b[38;5;28msorted\u001b[39m(special_tokens, key=\u001b[38;5;28mlen\u001b[39m, reverse=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      5\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m special_tokens_sorted:\n",
      "\u001b[31mNameError\u001b[39m: name 'List' is not defined"
     ]
    }
   ],
   "source": [
    "def split_by_special_tokens(text: str, special_tokens: list[str]) -> List[str]:\n",
    "    special_tokens_sorted = sorted(special_tokens, key=len, reverse=True)\n",
    "    if not special_tokens_sorted:\n",
    "        return [text]\n",
    "    pattern = \"|\".join(re.escape(t) for t in special_tokens_sorted)\n",
    "    special_chunks = re.split(f\"({pattern})\", text)\n",
    "\n",
    "    return special_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93085c6a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'List' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mword_to_bytes\u001b[39m(word: \u001b[38;5;28mstr\u001b[39m) -> \u001b[43mList\u001b[49m[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[32m      2\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03m    Convert a word to bytes.\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m      5\u001b[39m     byte_ids = [\u001b[38;5;28mbytes\u001b[39m([b]) \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m word.encode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m)]\n",
      "\u001b[31mNameError\u001b[39m: name 'List' is not defined"
     ]
    }
   ],
   "source": [
    "def word_to_bytes(word: str) -> List[bytes]:\n",
    "    \"\"\"\n",
    "    Convert a word to bytes.\n",
    "    \"\"\"\n",
    "    byte_ids = [bytes([b]) for b in word.encode(\"utf-8\")]\n",
    "\n",
    "    return byte_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db00bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab: Dict[int, bytes],\n",
    "        merges: List[Tuple[bytes, bytes]] = [],\n",
    "        special_tokens: List[str] = [],\n",
    "    ):\n",
    "        self.vocab = vocab\n",
    "        self.merges = merges\n",
    "        self.special_tokens = {token: i for i, token in enumerate(special_tokens, start=len(vocab))} if special_tokens else {}\n",
    "\n",
    "        self.vocab_inv = {v: k for k, v in vocab.items()}\n",
    "    \n",
    "    def _pre_tokenize(self, text) -> List[bytes]:\n",
    "        \"\"\"\n",
    "        Pre-tokenize the input text into bytes.\n",
    "        \"\"\"\n",
    "        parts = split_by_special_tokens(text, list(self.special_tokens.keys()))\n",
    "        token_list = []\n",
    "        \n",
    "        for part in parts:\n",
    "            if part in self.special_tokens.keys():\n",
    "                token_list.append(word_to_bytes(part))\n",
    "            else:\n",
    "                tokens = re.findall(PAT, part)\n",
    "                token_list.extend(word_to_bytes(token) for token in tokens)\n",
    "\n",
    "        return token_list\n",
    "        \n",
    "    \n",
    "    def encode(self, text: str) -> List[int]:\n",
    "        \n",
    "        # Pre-tokenize the input text into bytes\n",
    "\n",
    "    def decode(self, ids: list[int]) -> str:\n",
    "        \"\"\"\n",
    "        Decode a list of token IDs back to a string.\n",
    "        \"\"\"\n",
    "        tokens = b\"\".join(self.vocab.get(i, b\"\\xef\\xbf\\xbd\") for i in ids)\n",
    "        return tokens.decode(\"utf-8\", errors=\"replace\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
